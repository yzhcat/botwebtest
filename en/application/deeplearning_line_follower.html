
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="OriginBot智能机器人开源套件·古月居">
      
      
      
        <link rel="canonical" href="https://www.guyuehome.com/en/application/deeplearning_line_follower.html">
      
      
        <link rel="prev" href="cv_line_follower.html">
      
      
        <link rel="next" href="gazebo_line_follower.html">
      
      
      <link rel="icon" href="../../assets/img/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.11">
    
    
      
        <title>Visual Line Following (AI Deep Learning) - OriginBot智能机器人开源套件</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/css/extensions.css">
    
      <link rel="stylesheet" href="../../assets/css/simpleLightbox.min.css">
    
      <link rel="stylesheet" href="../../assets/css/custom.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="green" data-md-color-accent="green">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#visual-line-following-ai-deep-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="OriginBot智能机器人开源套件" class="md-header__button md-logo" aria-label="OriginBot智能机器人开源套件" data-md-component="logo">
      
  <img src="../../assets/img/favicon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            OriginBot智能机器人开源套件
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Visual Line Following (AI Deep Learning)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="green" data-md-color-accent="green"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="green" data-md-color-accent="green"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="deeplearning_line_follower.html" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../application/deeplearning_line_follower.html" hreflang="zh" class="md-select__link">
              简体中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://gitee.com/guyuehome/originbot" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    originbot
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="OriginBot智能机器人开源套件" class="md-nav__button md-logo" aria-label="OriginBot智能机器人开源套件" data-md-component="logo">
      
  <img src="../../assets/img/favicon.png" alt="logo">

    </a>
    OriginBot智能机器人开源套件
  </label>
  
    <div class="md-nav__source">
      <a href="https://gitee.com/guyuehome/originbot" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    originbot
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Project Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../guide/quick_guide.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    User Guide
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Kit Information
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Kit Information
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../material/material_list.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Kit List
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../material/open_source_link.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Resources Link
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../material/common_software.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Common Software
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../guide/hardware_setup.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hardware Assembly
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Software Configuration
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Software Configuration
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guide/image_install.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image Installation and Backup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guide/firmware_install.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Controller Firmware Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../guide/pc_config.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PC Environment Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../guide/easy_start.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quick Start
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Basic Usage
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Basic Usage
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/originbot_charging.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robot Charging Method
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/ide_setup.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Setting Up Development Environment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/package_develop.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Code Development Method
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/originbot_bringup.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robot Startup and Parameter Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/teleoperation.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robot Teleoperation and Visualization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/camera_visualization.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Camera Driver and Visualization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/lidar_visualization.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Lidar Driver and Visualization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/imu_visualization.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IMU Driver and Visualization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/parameter_config.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dynamic Monitoring of Robot Parameters
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/odom_calibration.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Robot Odometer Calibration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/protocol_description.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Communication Protocol Description
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/hmi.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HMI Control Description
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/originbot_freertos.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Real-Time Operating System RTOS Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/ekf.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    EKF Multi-Sensor Fusion Positioning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../manual/webviz.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    WebViz Visualization
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" checked>
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Application Features
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Application Features
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="basic_program.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basic Function Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="gazebo_simulation.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gazebo Virtual Simulation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="gazebo_slam.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SLAM Map Building (Gazebo)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="slam.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SLAM Map Building
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="gazebo_navigation.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autonomous Navigation (Gazebo)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="navigation.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Autonomous Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="body_detection.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Human Body Tracking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="gesture_control.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gesture Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="cv_line_follower.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Visual Line Following (OpenCV)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Visual Line Following (AI Deep Learning)
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="deeplearning_line_follower.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Visual Line Following (AI Deep Learning)
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#apply-goals" class="md-nav__link">
    <span class="md-ellipsis">
      Apply goals
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#run-method" class="md-nav__link">
    <span class="md-ellipsis">
      Run method
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Run method">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-description" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario description
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-visual-line-patrol" class="md-nav__link">
    <span class="md-ellipsis">
      Start the visual line patrol
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-robot" class="md-nav__link">
    <span class="md-ellipsis">
      Start the robot
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-to-the-principle" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction to the principle
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction to the principle">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deep-learning-process" class="md-nav__link">
    <span class="md-ellipsis">
      Deep learning process
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visual-line-inspection-process" class="md-nav__link">
    <span class="md-ellipsis">
      Visual line inspection process
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-labeling-and-training" class="md-nav__link">
    <span class="md-ellipsis">
      Data labeling and training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deployment-on-the-board-and-use" class="md-nav__link">
    <span class="md-ellipsis">
      Deployment on the board and use
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ai-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      AI deep learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AI deep learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pc-environment-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      PC environment configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PC environment configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      Install pytorch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-docker" class="md-nav__link">
    <span class="md-ellipsis">
      Install docker
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#introduction-to-related-feature-packs" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction to related feature packs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-collection-and-annotation" class="md-nav__link">
    <span class="md-ellipsis">
      Data collection and annotation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Data collection and annotation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#start-the-originbot-camera" class="md-nav__link">
    <span class="md-ellipsis">
      Start the OriginBot camera
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-data-collection-and-annotation-program" class="md-nav__link">
    <span class="md-ellipsis">
      Start the data collection and annotation program
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-collection-and-annotation_1" class="md-nav__link">
    <span class="md-ellipsis">
      Data collection and annotation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-training" class="md-nav__link">
    <span class="md-ellipsis">
      Model training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Model selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-training_1" class="md-nav__link">
    <span class="md-ellipsis">
      Model training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-conversion" class="md-nav__link">
    <span class="md-ellipsis">
      Model conversion
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model conversion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generate-an-onnx-model" class="md-nav__link">
    <span class="md-ellipsis">
      Generate an onnx model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-ai-toolchain-docker" class="md-nav__link">
    <span class="md-ellipsis">
      Start the AI toolchain docker
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibration-data-is-generated" class="md-nav__link">
    <span class="md-ellipsis">
      Calibration data is generated
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-model-is-compiled-to-generate-a-fixed-point-model" class="md-nav__link">
    <span class="md-ellipsis">
      The model is compiled to generate a fixed-point model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-deployment" class="md-nav__link">
    <span class="md-ellipsis">
      Model deployment
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model deployment">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-deployment_1" class="md-nav__link">
    <span class="md-ellipsis">
      Model deployment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-camera" class="md-nav__link">
    <span class="md-ellipsis">
      Start the camera
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-bot" class="md-nav__link">
    <span class="md-ellipsis">
      Start the bot
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="gazebo_line_follower.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Visual Line Following (Gazebo)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="parking_search.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Parking Spot Finding (AI Deep Learning)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="play_football.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Playing Football
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="tracking.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Trajectory Tracking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="take_pictures.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image Acquisition
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="qrcode_detection.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    QR Code Detection and Tracking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="vlpr.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    License Plate Recognition
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="mobilesam.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Segment Everything
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    More Content
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            More Content
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reference/contributing.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contributing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reference/changelog.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Changelog
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://www.guyuehome.com/interlocution?id=1826932316801544194" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Community Exchange
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../about.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About Us
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#apply-goals" class="md-nav__link">
    <span class="md-ellipsis">
      Apply goals
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#run-method" class="md-nav__link">
    <span class="md-ellipsis">
      Run method
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Run method">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-description" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario description
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-visual-line-patrol" class="md-nav__link">
    <span class="md-ellipsis">
      Start the visual line patrol
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-robot" class="md-nav__link">
    <span class="md-ellipsis">
      Start the robot
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-to-the-principle" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction to the principle
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction to the principle">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deep-learning-process" class="md-nav__link">
    <span class="md-ellipsis">
      Deep learning process
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visual-line-inspection-process" class="md-nav__link">
    <span class="md-ellipsis">
      Visual line inspection process
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-labeling-and-training" class="md-nav__link">
    <span class="md-ellipsis">
      Data labeling and training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deployment-on-the-board-and-use" class="md-nav__link">
    <span class="md-ellipsis">
      Deployment on the board and use
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ai-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      AI deep learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AI deep learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pc-environment-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      PC environment configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PC environment configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      Install pytorch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-docker" class="md-nav__link">
    <span class="md-ellipsis">
      Install docker
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#introduction-to-related-feature-packs" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction to related feature packs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-collection-and-annotation" class="md-nav__link">
    <span class="md-ellipsis">
      Data collection and annotation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Data collection and annotation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#start-the-originbot-camera" class="md-nav__link">
    <span class="md-ellipsis">
      Start the OriginBot camera
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-data-collection-and-annotation-program" class="md-nav__link">
    <span class="md-ellipsis">
      Start the data collection and annotation program
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-collection-and-annotation_1" class="md-nav__link">
    <span class="md-ellipsis">
      Data collection and annotation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-training" class="md-nav__link">
    <span class="md-ellipsis">
      Model training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Model selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-training_1" class="md-nav__link">
    <span class="md-ellipsis">
      Model training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-conversion" class="md-nav__link">
    <span class="md-ellipsis">
      Model conversion
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model conversion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generate-an-onnx-model" class="md-nav__link">
    <span class="md-ellipsis">
      Generate an onnx model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-ai-toolchain-docker" class="md-nav__link">
    <span class="md-ellipsis">
      Start the AI toolchain docker
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#calibration-data-is-generated" class="md-nav__link">
    <span class="md-ellipsis">
      Calibration data is generated
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-model-is-compiled-to-generate-a-fixed-point-model" class="md-nav__link">
    <span class="md-ellipsis">
      The model is compiled to generate a fixed-point model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-deployment" class="md-nav__link">
    <span class="md-ellipsis">
      Model deployment
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model deployment">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-deployment_1" class="md-nav__link">
    <span class="md-ellipsis">
      Model deployment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-camera" class="md-nav__link">
    <span class="md-ellipsis">
      Start the camera
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-the-bot" class="md-nav__link">
    <span class="md-ellipsis">
      Start the bot
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="visual-line-following-ai-deep-learning"><strong>Visual Line Following (AI Deep Learning)</strong></h1>
<details class="hint" open="open">
<summary>Hint</summary>
<p>The operating environment and software and hardware configurations are as follows:</p>
<ul>
<li>OriginBot Pro</li>
<li>PC：Ubuntu (≥22.04) + ROS2 (≥humble) + pytorch + docker</li>
<li>Line following scene: black path line, with obvious contrast with the background</li>
<li>RDK X3 Please refer to <a href="deeplearning_line_follower_x3.html">Visual Line Following (AI Deep Learning)_x3</a></li>
</ul>
</details>
<h2 id="apply-goals"><strong>Apply goals</strong></h2>
<p>In the <a href="cv_line_follower.html" target="_blank">visual line patrol（OpenCV）</a>, we can already let the car follow the black path line movement, to achieve the most basic visual line patrol task, but you may have found that the image recognition based on OpenCV is greatly affected by the light, after changing the site, the threshold needs to be readjusted, is it possible to let the robot autonomously adapt to the changes in the environment? That is, let the robot learn on its own.</p>
<p><img alt="deeplearning_follow_line" src="../../assets/img/deeplearning_line_follower/deeplearning_follow_line.gif" /></p>
<p>No problem, everyone must have heard of deep learning, using this method, we can achieve a data-driven visual line patrol effect, what kind of site needs to be adapted, just need to collect some picture data, and then we will try it.</p>
<h2 id="run-method"><strong>Run method</strong></h2>
<h3 id="scenario-description"><strong>Scenario description</strong></h3>
<p>In the application code of OriginBot, we have trained a set of line patrol models for you, using the following scenarios:</p>
<p><img alt="0d09ef8f7eaeb842f4a4b6d67c2d07f" class="img-fluid" src="../../assets/img/deeplearning_line_follower/0d09ef8f7eaeb842f4a4b6d67c2d07f.jpg" tag="1" /></p>
<p>If you have a similar environment around you, you can skip the machine learning steps mentioned above for the time being and go directly to the deployment process, let's first try how effective the visual line patrol is using deep learning.</p>
<h3 id="start-the-visual-line-patrol"><strong>Start the visual line patrol</strong></h3>
<p>First, place the OriginBot in the scene where you are patrolling the line, and adjust the camera angle down as much as possible to avoid external interference.</p>
<p><img alt="e15c780e42a2fc9bb4895f6425a9275" class="img-fluid" src="../../assets/img/deeplearning_line_follower/e15c780e42a2fc9bb4895f6425a9275.jpg" tag="1" /></p>
<p>Next, start the line patrol function, and note the parameters that need to be followed by the model path and name after the command:</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nb">cd</span><span class="w"> </span>/userdata/dev_ws/src/originbot/originbot_deeplearning/line_follower_perception/
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>ros2<span class="w"> </span>run<span class="w"> </span>line_follower_perception<span class="w"> </span>line_follower_perception<span class="w"> </span>--ros-args<span class="w"> </span>-p<span class="w"> </span>model_path:<span class="o">=</span>model/resnet18_224x224_nv12.bin<span class="w"> </span>-p<span class="w"> </span>model_name:<span class="o">=</span>resnet18_224x224_nv12
</code></pre></div>
<img alt="e15c780e42a2fc9bb4895f6425a9275" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-15_21-28.png" tag="1" /></p>
<h3 id="start-the-robot"><strong>Start the robot</strong></h3>
<p>After SSH is connected to OriginBot, enter the following command in the terminal to start the robot chassis and camera:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>ros2<span class="w"> </span>launch<span class="w"> </span>line_follower_perception<span class="w"> </span>usb_cam_web.launch.py
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>ros2<span class="w"> </span>launch<span class="w"> </span>originbot_bringup<span class="w"> </span>originbot.launch.py
</code></pre></div>
<p>After the camera is successfully started, you can see the dynamically recognized path line position in the line patrol terminal:</p>
<p><img alt="image-20220822151357113" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-15_21-30.png" tag="1" /></p>
<p>After running for a while, you can see that the robot has started to patrol the line.</p>
<p><img alt="deeplearning_follow_line" src="../../assets/img/deeplearning_line_follower/deeplearning_follow_line.gif" /></p>
<p>Deep learning is based on data, the robot can only learn from the scene that it has seen in the data. If it has not seen the scene, the recognition effect is not very good. So if we use the patrol color and the surrounding environment are different, we still need to collect as much data as possible so that the robot can learn. The complete deep learning process can refer to the following.</p>
<h2 id="introduction-to-the-principle"><strong>Introduction to the principle</strong></h2>
<p>The perception of the position of the guide line in the line-tracking task is actually the perception of the characteristics of the guide line. The photoelectric sensor detects the position of the line by using the difference in reflected light. Different colors of surfaces have different reflectivity. For example, white has a high reflectivity and black absorbs light, so the received signal strength is different. The traditional image processing method is to extract features based on the pixel values of the image, while deep learning automatically learns the image features through the neural network and returns the position of the features in the image.</p>
<p><img alt="line_flow" src="../../assets/img/deeplearning_line_follower/line_flow.png" /></p>
<p>deeplearning in the line-tracking task is actually a powerful feature matcher. It can extract guide line features from the image and return the position of the features in the image. As long as a certain number of labeled data are prepared, a deep learning model can be trained to achieve guide line position perception, and there is no need to manually adjust the threshold.</p>
<h3 id="deep-learning-process"><strong>Deep learning process</strong></h3>
<p>Compared with traditional image processing, deep learning can enable machine vision to adapt to more changes, thereby improving the accuracy of complex environments. Before we begin, let's briefly introduce the basic process of deep learning.</p>
<p><img alt="image-20220915112906511" class="img-fluid" src="../../assets/img/deeplearning_line_follower/image-20220915112906511.png" tag="1" /></p>
<p>The core purpose of machine learning is to help us solve problems, which can be broken down into six main steps:</p>
<ul>
<li><strong>Problem definition</strong>：What is the problem we are trying to solve? For example, in the case of visual line patrol here, it is necessary to identify the position of the line in the image.</li>
<li><strong>Data preparation</strong>：Start preparing your data for the problem you want to solve. For example, you need to prepare photos of various line patrol scenes for machine learning.</li>
<li><strong>Model selection/development</strong>：A model is a set of processes for processing data, which is often referred to as CNN convolutional neural networks, GAN generative adversarial networks, RNN recurrent neural networks, and so on.</li>
<li><strong>Model training and tuning</strong>：Putting data into the model and training the optimal parameters can be understood as the process of machine learning.</li>
<li><strong>Model evaluation test</strong>：Just like a quiz, we take some data and give it to the trained model to see how it works.</li>
<li><strong>Deployment</strong>：After everything is ready, you can put the trained model on the robot, that is, officially transfer the knowledge to a certain robot, and it can solve the problem raised earlier.</li>
</ul>
<h3 id="visual-line-inspection-process"><strong>Visual line inspection process</strong></h3>
<p><img alt="deeplearning_struct" class="img-fluid" src="../../assets/img/deeplearning_line_follower/deeplearning_struct.jpg" tag="1" /></p>
<p>The OriginBot has two active wheels, one passive wheel, and the two active wheels are used to rotate the robot through differential speed. The MCU module is mainly used for motor control of the robot and communicates with the main control RDK X5 through UART. The main controller is selected to be the earthmelon RDK X5, which has 10T computing power and can easily handle complex CNN inference tasks.</p>
<p>The whole system is shown in the figure above. The RDK X5 obtains the data of the environment in front of the car through the camera, and the image data is inferred by the trained CNN model to obtain the coordinate value of the guide line, and then the motion mode of the car is calculated according to a certain control strategy, and the motion control command is issued to the car through UART to realize the closed-loop control of the whole system.</p>
<p>The PC is used for data annotation and training.</p>
<h3 id="data-labeling-and-training"><strong>Data labeling and training</strong></h3>
<p>ResNet18 is a 18-layer deep residual convolutional neural network, one of the most classic networks in the deep learning field, and has excellent performance in image classification tasks. It solves the problem of gradient disappearance through residual connections, allowing deeper neural networks to be successfully trained. On the D-Robotics RDK, ResNet18 inference performance reaches 232FPS, ensuring the real-time processing of data.</p>
<p>ResNet18 inputs a resolution of 224x224, so the image size passed in for training needs to be transformed to 224x224</p>
<p><img alt="img_cut" src="../../assets/img/deeplearning_line_follower/img_cut.png" /></p>
<p>In the line-tracking scenario, the part that is more concerned about is the lower part of the image, so the part that is not needed can be cut off synchronously during annotation, and the line height can be cut off to 224.</p>
<p>For the width, you can use scaling. The image size captured by the USB camera is 640x480, and the width will be scaled to 224 during training. This step usually does not modify the original dataset file, but scales it when reading.</p>
<p><img alt="imgdata_tran" src="../../assets/img/deeplearning_line_follower/imgdata_tran.png" /></p>
<p>Summarizing the process, the USB camera captures a series of 640x480 images with guiding lines; the annotation program reads an image, crops part of the area to get a 640x224 image, and completes the annotation by clicking the position of the guiding line with the mouse; the obtained dataset is passed to ResNet18 before scaling the annotated coordinates and images to 224x224 to start training, and the model file is generated.</p>
<h3 id="deployment-on-the-board-and-use"><strong>Deployment on the board and use</strong></h3>
<p>The floating-point model <em>.pth file trained by pytorch will have very low efficiency if run directly on the D-Robotics RDK. In order to improve the operational efficiency and exert the 10T computing power of the BPU, the operation of convert the floating-point model to the fixed-point model is required here. The trained model is first converted.onnx(</em><em>Open Neural Network Exchange</em><em>) - an open format used to represent deep learning models, allowing models between different frameworks to be converted and used mutually. Generate fixed-point models that can run on the D-Robotics RDK BPU </em>.bin file.</p>
<p>Load the model file on the board, and capture images in real time, crop and scale them, and then input them into the ResNet network to get the position of the guiding line in the image.</p>
<p><img alt="Line inspection process" src="../../assets/img/deeplearning_line_follower/dlf.png" /></p>
<p>However, the obtained coordinates at this time are based on 224x224, so the coordinates need to be restored to 640x480. The horizontal coordinate is x*(640/224), and the vertical coordinate is y+ the cropped height. After that, control can be achieved based on the position of the guide line. If the guide line is on the left, the car will turn left accordingly; if it is on the right, the car will turn right accordingly.</p>
<h2 id="ai-deep-learning"><strong>AI deep learning</strong></h2>
<p><img alt="deeplearning_flow" class="img-fluid" src="../../assets/img/deeplearning_line_follower/deeplearning_flow.jpg" tag="1" /></p>
<p>The next deep learning operation is mainly divided into the above steps, before the operation, you also need to complete the configuration of the PC environment, let's experience deep learning together.</p>
<h3 id="pc-environment-configuration"><strong>PC environment configuration</strong></h3>
<p>The basic environment on the PC side is Ubuntu+ROS+pytorch+docker, please complete the configuration of Ubuntu, ROS, and OriginBot function packages according to  <a href="../guide/pc_config.html" target="_blank">Desktop environment configuration</a>.</p>
<h4 id="install-pytorch"><strong>Install pytorch</strong></h4>
<p>The training framework is pytorch, and you can directly use the following command to install it on the Ubuntu system on the PC side:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="c1"># create a virtual environment</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>python3<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>~/.pytorch_venv
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="nb">source</span><span class="w"> </span>~/.pytorch_venv/bin/activate
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="c1"># install pytorch</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>pip3<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>--extra-index-url<span class="w"> </span>https://download.pytorch.org/whl/cpu
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span class="c1"># install opencv</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>pip3<span class="w"> </span>install<span class="w"> </span>opencv-python
</code></pre></div>
<details class="hint" open="open">
<summary>Hint</summary>
<p>To keep the system environment clean, a virtual environment is created for pytorch, and the environment is activated using the command <code>source ~/.pytorch_venv/bin/activate</code>, and the activation is canceled using the command <code>deactivate</code>.
In order to ensure universality, the CPU version of pytorch is installed here. If there is a GPU on the hardware, the GPU version of pytorch can also be selected, which will be faster in training speed.</p>
</details>
<h4 id="install-docker"><strong>Install docker</strong></h4>
<p>Docker is required for subsequent model conversion to save time for environment configuration, and you need to install it first and download the image used for model conversion.</p>
<p>The installation of Docker in the Ubuntu environment can be completed by referring to the following website or online information:</p>
<p><a href="https://www.runoob.com/docker/ubuntu-docker-install.html" target="_blank">Ubuntu Docker installation</a></p>
<p>After the installation is complete, you can use the following command to download the toolchain and docker image for subsequent model conversions:</p>
<ul>
<li>
<p>OE Package (1.28GB)   <br />
horizon_x5_open_explorer_v1.2.6-py310_20240724.tar.gz
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>wget<span class="w"> </span>-c<span class="w"> </span>ftp://x5ftp@vrftp.horizon.ai/OpenExplorer/v1.2.6_release/horizon_x5_open_explorer_v1.2.6-py310_20240724.tar.gz<span class="w"> </span>--ftp-password<span class="o">=</span>x5ftp@123$%
</code></pre></div></p>
</li>
<li>
<p>Ubuntu20.04 CPU Docker (1.65GB)     <br />
docker_openexplorer_ubuntu_20_x5_cpu_v1.2.8.tar.gz
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>wget<span class="w"> </span>-c<span class="w"> </span>ftp://x5ftp@vrftp.horizon.ai/OpenExplorer/v1.2.8_release/docker_openexplorer_ubuntu_20_x5_cpu_v1.2.8.tar.gz<span class="w"> </span>--ftp-password<span class="o">=</span>x5ftp@123$%
</code></pre></div></p>
</li>
</ul>
<details class="hint" open="open">
<summary>Hint</summary>
<p>The above docker images are large in size, please ensure that the network is smooth and wait patiently.</p>
</details>
<h4 id="introduction-to-related-feature-packs"><strong>Introduction to related feature packs</strong></h4>
<p>Throughout the deep learning process, we will use three directly related feature packages/folders:</p>
<ul>
<li>
<p><strong>10_model_convert</strong>：Used to store floating-point model to fixed-point model related code and configuration, and all the contents of this folder need to be run in the AI toolchain docker;</p>
</li>
<li>
<p><strong>line_follower_model</strong>：Used to store model training-related content, such as data labeling, model training, and generating onnx models, all the contents of this folder are run on the PC;</p>
</li>
<li>
<p><strong>line_follower_perception</strong>：Used to store the code of the line following program, which can be compiled on the board or generated by cross-compiling the package running on RDK.</p>
</li>
</ul>
<p>Code repository: <a href="https://github.com/D-Robotics/line_follower.git">https://github.com/D-Robotics/line_follower.git</a>, you can get the code using the following command:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>mkdir<span class="w"> </span>-p<span class="w"> </span>~/line_follower_ws/src
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="nb">cd</span><span class="w"> </span>~/line_follower_ws/src/
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/yzhcat/line_follower.git<span class="w"> </span>-b<span class="w"> </span>feature-x5-usb_cam
</code></pre></div>
<p>Once the environment is ready, we can get started.</p>
<h3 id="data-collection-and-annotation"><strong>Data collection and annotation</strong></h3>
<p>Data is the foundation of deep learning, we first need to complete the collection and annotation of data, this process is mainly completed on the PC side, you need to subscribe to the real-time images of the OriginBot robot side, the main process is as follows:</p>
<p><img alt="data_flow" class="img-fluid" src="../../assets/img/deeplearning_line_follower/data_flow.jpg" tag="1" /></p>
<p>（1）Start the OriginBot camera, collect images of the patrol scene, and publish the image topic image_raw;</p>
<p>（2）The PC side subscribes to the image topics published by OriginBot and obtains image data;</p>
<p>（3）The PC side crops the image, and manually completes the annotation of the path line in each image;</p>
<p>（4）Save the annotation result and the corresponding image.</p>
<h4 id="start-the-originbot-camera"><strong>Start the OriginBot camera</strong></h4>
<p>After connecting to OriginBot via SSH, enter the following command in the terminal to start the camera of OriginBot:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>ros2<span class="w"> </span>launch<span class="w"> </span>hobot_usb_cam<span class="w"> </span>hobot_usb_cam_websocket.launch.py
</code></pre></div>
<p>Use the browser to enter <a href="http://ip:8000/">http://IP:8000</a> to view the image (IP is the device IP address).</p>
<blockquote>
<p>The static IP address of the network cable is 192.168.127.10
Other connection methods can use the 'ifconfig' or 'ip addr' command to view the ip</p>
</blockquote>
<details class="hint" open="open">
<summary>Hint</summary>
<p>Data collection process can manually set the posture of the robot, and the robot chassis is not started, only the camera is started.</p>
</details>
<h4 id="start-the-data-collection-and-annotation-program"><strong>Start the data collection and annotation program</strong></h4>
<div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><input id="__tabbed_1_2" name="__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="__tabbed_1_1">Real-time</label><label for="__tabbed_1_2">annotation after collection</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<p>On the PC side, use the following command to start the data acquisition and annotation program in the line_follower_model package:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="nb">cd</span><span class="w"> </span>~/line_follower_ws/
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>colcon<span class="w"> </span>build<span class="w"> </span>--symlink-install<span class="w"> </span>--packages-select<span class="w"> </span>line_follower_model
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="nb">source</span><span class="w"> </span>install/setup.bash
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a>ros2<span class="w"> </span>run<span class="w"> </span>line_follower_model<span class="w"> </span>annotation
</code></pre></div>
<details class="hint" open="open">
<summary>Hint</summary>
<ul>
<li>The program subscribes to the sensor_msgs/Image type image topic /image</li>
<li>If you are prompted that cv-bridge is not found, you can try the following command to install
sudo apt install ros-$ROS_DISTRO-cv-bridge</li>
</ul>
</details>
</div>
<div class="tabbed-block">
<p>Use MobaXterm SSH to connect to OriginBot successfully, enter the following command in the terminal to save the image
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="c1"># install image_view</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>ros-humble-image-view
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="c1"># create a folder to save the image</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>mkdir<span class="w"> </span>input_images<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>input_images
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a><span class="c1"># run image_view</span>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>ros2<span class="w"> </span>run<span class="w"> </span>image_view<span class="w"> </span>image_view
</code></pre></div></p>
<blockquote>
<p>image_view using the right mouse button to save the image, the saved path is in the current directory, you can use the <code>pwd</code> command to view the current directory</p>
</blockquote>
<p><img alt="image_view" src="../../assets/img/deeplearning_line_follower/image_view_save.png" /></p>
<p><img alt="mobaxterm_out_file" src="../../assets/img/deeplearning_line_follower/mobaxterm_out_file.png" /></p>
<p>Export the collected images to the PC side, and then copy them to the <code>~/line_follower_ws</code> directory.
The image data folder is named input_images, and the line_follower_model package data collection annotation program is started using the following command:</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="nb">cd</span><span class="w"> </span>~/line_follower_ws/
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="nb">source</span><span class="w"> </span>install/setup.bash
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>ros2<span class="w"> </span>run<span class="w"> </span>line_follower_model<span class="w"> </span>pic_annotation
</code></pre></div>
<img alt="pic_annotation_member" src="../../assets/img/deeplearning_line_follower/pic_annotation_member.png" /></p>
<details class="hint" open="open">
<summary>Hint</summary>
<ul>
<li>The annotation script does not depend on the ROS environment, you can put the annotation script and image data in the same directory and start it directly through <code>python3 pic_annotation_member.py</code></li>
<li>The annotation script is located in <code>~/line_follower_ws/src/line_follower/line_follower_model/line_follower_model/</code></li>
</ul>
</details>
</div>
</div>
</div>
<h4 id="data-collection-and-annotation_1"><strong>Data collection and annotation</strong></h4>
<p>After successful startup, press the enter key on the keyboard, the program will subscribe to the latest image topic, crop and display it through a visual window, and the data collection is successful:</p>
<p><img alt="8057f0fb-9925-4b20-b12c-64574956e92a" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-15_21-47.png" tag="1" /></p>
<p>Click the center of the path line in the vertical direction of the picture with the left mouse button to complete the annotation of the image data of the frame:</p>
<p><img alt="87b8418b-cbc0-48c9-ba0f-3187e364bc64" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-15_21-47_1.png" tag="1" /></p>
<p>Press the Enter key to automatically save the image to the image_dataset folder under the current path, and save the markup result. The image is named as follows:</p>
<blockquote>
<p>xy_[x coordinates]<em>[y coordinates]</em>[uuid].jpg</p>
</blockquote>
<p>The uuid is the unique identifier of the image to avoid the same file name.</p>
<details class="hint" open="open">
<summary>Hint</summary>
<p>If the click is inaccurate, you can click the left mouse button several times until you are satisfied. If you do not click the annotation, the image will be skipped after press enter and will not be put into the dataset.</p>
</details>
<p><img alt="20220915183911" class="img-fluid" src="../../assets/img/deeplearning_line_follower/20220915183911.jpg" tag="1" /></p>
<p>Continuously adjust the position of the robot in the scene, consider various possible image effects, complete the above data collection and annotation process in cycles, and collect a sufficient amount of data, at least 100 pieces are recommended for subsequent model training. When the environment or site changes, the corresponding images can also be collected and trained together to improve the adaptability of the model.</p>
<p><img alt="image" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-15_22-19.png" tag="1" /></p>
<h3 id="model-training"><strong>Model training</strong></h3>
<h4 id="model-selection"><strong>Model selection</strong></h4>
<p>Convolutional Neural Network（Convolutional Neural Network, CNN） is a deep neural network model that is widely used in the fields of image and natural language processing. In 1998, Lecun et al. proposed a gradient-based backpropagation algorithm for document recognition. In this neural network, the convolutional layer plays a crucial role.With the continuous enhancement of computing power, some large CNN networks began to show great advantages in the image field. In 2012, Krizhevsky et al. proposed the AlexNet network structure, and won the championship in the ImageNet image classification competition by 11%. Subsequently, different scholars proposed a series of network structures and continuously refreshed the results of ImageNet, among which the more classic networks include: VGG(Visual Geometry Group), GoogLeNet and ResNet. The convolutional neural network consists of an input layer, a convolutional layer, a pooling layer, a fully connected layer, and an output layer, and its structure is as follows:</p>
<p><img alt="fc0d7f6e-2c26-42df-bbbf-9a1ac02c47b5" class="img-fluid" src="../../assets/img/deeplearning_line_follower/fc0d7f6e-2c26-42df-bbbf-9a1ac02c47b5.png" tag="1" /></p>
<p>Considering the maturity of the model and the hardware requirements of the CPU/GPU for training the model, the ResNet network is selected as the backbone. The Residual Neural Network (ResNet) was proposed by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun and others from Microsoft Research, and won the championship in the 2015 ILSVRC (ImageNet Large Scale Visual Recognition Challenge). ResNet cleverly uses shortcut connections to solve the problem of model degradation in deep networks, and is one of the most widely used CNN feature extraction networks. The structure of ResNet18 is as follows:</p>
<p><img alt="8f27f7d6-97a3-4b9d-bff7-bf2d7652d2bf" class="img-fluid" src="../../assets/img/deeplearning_line_follower/8f27f7d6-97a3-4b9d-bff7-bf2d7652d2bf.png" tag="1" /></p>
<p>The ResNet18 inference performance is up to 232 FPS on the RDK X3 and the ResNet50 inference performance is also over 100 FPS.The high frame rate ensures real-time data processing and is a necessary condition for subsequent speed increases and more complex applications. Here, the ResNet18 network structure is used first, and the deeper ResNet50 network structure is considered when the bottleneck is encountered later. In order to satisfy the output guide line coordinate value x,y, you need to modify the ResNet18 network FC output to 2, that is, directly output the x,y coordinate value of the guide line. The ResNet18 input resolution is 224x224.</p>
<h4 id="model-training_1"><strong>Model training</strong></h4>
<p>The above-mentioned models can be directly reused from the definitions in PyTorch, and the sharding of the dataset and the training of the model are all encapsulated in the code of the line_follower_model feature package.</p>
<p><img alt="model_traning" class="img-fluid" src="../../assets/img/deeplearning_line_follower/model_traning.jpg" tag="1" /></p>
<p>Directly activate the pytorch environment on the PC and run the following command to start training:</p>
<p>activate pytorch environment
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="nb">source</span><span class="w"> </span>~/.pytorch_venv/bin/activate
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$PYTHONPATH</span>:<span class="k">$(</span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import sys; print(&#39;:&#39;.join(sys.path))&quot;</span><span class="k">)</span>
</code></pre></div></p>
<p>run training script
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="nb">cd</span><span class="w"> </span>~/line_follower_ws/
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a><span class="nb">source</span><span class="w"> </span>install/setup.bash
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>ros2<span class="w"> </span>run<span class="w"> </span>line_follower_model<span class="w"> </span>training
</code></pre></div></p>
<p><img alt="model_traning" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-15_17-33.png" tag="1" /></p>
<p>The model training process takes a long time, about 30 minutes, please be patient.</p>
<p>After training is complete, the .pth model file will be generated.</p>
<p><img alt="model_traning" src="../../assets/img/deeplearning_line_follower/model_tran.png" /></p>
<details class="hint" open="open">
<summary>Hint</summary>
<ul>
<li>The training script does not depend on the ros environment, you can put the training script and image data in the same directory, activate the pytorch environment and start it directly through python <code>python3 training_member_function.py</code></li>
<li>The training script is in <code>~/line_follower_ws/src/line_follower/line_follower_model/line_follower_model/</code></li>
</ul>
</details>
<h3 id="model-conversion"><strong>Model conversion</strong></h3>
<p>The floating-point model trained by pytorch would be inefficient if run directly on RDK X3. In order to improve the operation efficiency and give full play to the 5T computing power of the BPU, the floating-point model conversion operation needs to be performed here.</p>
<p><img alt="model_transform" class="img-fluid" src="../../assets/img/deeplearning_line_follower/model_transform.jpg" tag="1" /></p>
<h4 id="generate-an-onnx-model"><strong>Generate an onnx model</strong></h4>
<p>On the PC side, use generate_onnx to convert the previously trained model into an onnx model by using the following command:</p>
<p>activate pytorch environment
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="nb">source</span><span class="w"> </span>~/.pytorch_venv/bin/activate
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$PYTHONPATH</span>:<span class="k">$(</span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import sys; print(&#39;:&#39;.join(sys.path))&quot;</span><span class="k">)</span>
</code></pre></div></p>
<p>run generate_onnx script
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="nb">cd</span><span class="w"> </span>~/line_follower_ws/
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a><span class="nb">source</span><span class="w"> </span>install/setup.bash
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>ros2<span class="w"> </span>run<span class="w"> </span>line_follower_model<span class="w"> </span>generate_onnx
</code></pre></div></p>
<p>running the above command will generate the best_line_follower_model_xy.onnx model in the current directory.</p>
<p><img alt="model_transform" src="../../assets/img/deeplearning_line_follower/model_onnx.png" /></p>
<details class="hint" open="open">
<summary>Hint</summary>
<ul>
<li>The training script does not depend on the ros environment, you can put the training script and image data in the same directory, activate the pytorch environment and start it directly through python <code>python3 generate_onnx_member_function.py</code></li>
<li>The training script is in <code>~/line_follower_ws/src/line_follower/line_follower_model/line_follower_model/</code></li>
</ul>
</details>
<h4 id="start-the-ai-toolchain-docker"><strong>Start the AI toolchain docker</strong></h4>
<p>Load the offline image into the local environment.
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>sudo<span class="w"> </span>docker<span class="w"> </span>load<span class="w"> </span>-i<span class="w"> </span>docker_openexplorer_xxx.tar.gz
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>sudo<span class="w"> </span>docker<span class="w"> </span>image<span class="w"> </span>ls
</code></pre></div>
<img alt="docker_images" src="../../assets/img/deeplearning_line_follower/docker_images.png" /></p>
<p>extract OE package horizon_x5_open_explorer_v1.2.8-py310_20240926.tar.gz</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>tar<span class="w"> </span>-xzvf<span class="w"> </span>horizon_x5_open_explorer_v1.2.6-py310_20240724.tar.gz
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a><span class="nb">cd</span><span class="w"> </span>horizon_x5_open_explorer_v1.2.6-py310_20240724/
</code></pre></div>
<p>Modify run_docker.sh. At the beginning, find version=v1.2.6 and replace it with version=v1.2.6-py10</p>
<p><img alt="修改run_docker" src="../../assets/img/deeplearning_line_follower/run_docker.png" /></p>
<p>Configure the 10_model_convert folder
Open the /10_model_convert folder, and put the image data image_dataset and the converted best_line_follower_model_xy.onnx model file into the 10_model_convert/mapper folder</p>
<p><img alt="10_model_convert" src="../../assets/img/deeplearning_line_follower/10_model_convert.png" /></p>
<p>copy 10_model_convert folder to samples/ai_toolchain/horizon_model_convert_sample/03_classification/ directory.
<img alt="image" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-15_17-23.png" tag="1" /></p>
<p>Then go back to the root directory of the OE package and load the AI toolchain docker image:</p>
<p>Run Docker</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a>sudo<span class="w"> </span>bash<span class="w"> </span>run_docker.sh<span class="w"> </span>./data/<span class="w"> </span>cpu
</code></pre></div>
<p><img alt="enter_docker" class="img-fluid" src="../../assets/img/deeplearning_line_follower/enter_docker.png" tag="1" /></p>
<p>enter the 10_model_convert folder</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="nb">cd</span><span class="w"> </span>samples/ai_toolchain/horizon_model_convert_sample/03_classification/10_model_convert/mapper
</code></pre></div>
<details class="hint" open="open">
<summary>Hint</summary>
<p>If you encounter the error Unable to find image 'openexplorer/ai_toolchain_ubuntu_20_x5_cpu:v1.2.6' locally, check whether the TAG of ai_toolchain_ubuntu_20_x5_cpu is v1.2.6-py10 through the <code>docker image ls</code> command. If not, replace version=v1.2.6 in the beginning of run_docker.sh with version=v1.2.6-py10.</p>
</details>
<h4 id="calibration-data-is-generated"><strong>Calibration data is generated</strong></h4>
<p>The calibration data generated in this step is mainly used for calibration during model compilation, and can use part of the training model data, as long as the standard is correct, the number is about 100, and we have copied the dataset in the previous step.</p>
<p>In the Docker image started, complete the following operations:</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>bash<span class="w"> </span>02_preprocess.sh
</code></pre></div>
<img alt="image" class="img-fluid" src="../../assets/img/deeplearning_line_follower/02_preprocess.png" tag="1" /></p>
<h4 id="the-model-is-compiled-to-generate-a-fixed-point-model"><strong>The model is compiled to generate a fixed-point model</strong></h4>
<p>This step generates a fixed-point model file for subsequent deployment on the robot:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a>bash<span class="w"> </span>03_build.sh
</code></pre></div>
<p><img alt="image" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-16_17-25.png" tag="1" /></p>
<p>After the compilation is successful, the final model file will be generated in the 10_model_output path.
<img alt="image" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-16_17-27.png" tag="1" /></p>
<p>We can right-click to copy the model file and put it in the line_follower_model feature pack for later deployment.
<img alt="image" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-15_18-15.png" tag="1" /></p>
<h3 id="model-deployment"><strong>Model deployment</strong></h3>
<p>The fixed-point model that can run on the RDK X3BPU has been obtained through the previous model transformation. How to deploy it on the RDK X3 to achieve the complete set of functions of image acquisition, model inference, and motion control? This is based on the Hobot DNN in TogetheROS. Hobot DNN is a board-end algorithm inference framework in the TogetheROS software stack, which uses BPU processors to implement AI inference functions on Horizon RDK X3, providing a simpler and easy-to-use model integrated development interface for robot application development, including model management, input processing and result analysis based on model description, and model output memory allocation management.</p>
<h4 id="model-deployment_1"><strong>Model deployment</strong></h4>
<p>Copy the generated fixed-point model resnet18_224x224_nv12.bin to the model folder in the line_follower_perception feature package of OriginBot, replace the original model, and recompile the workspace.</p>
<p>After the compilation is complete, you can run the following command to deploy the model, where the parameters model_path and model_name specify the path and name of the model:</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="nb">cd</span><span class="w"> </span>/userdata/dev_ws/src/originbot/originbot_deeplearning/line_follower_perception/
<a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a>ros2<span class="w"> </span>run<span class="w"> </span>line_follower_perception<span class="w"> </span>line_follower_perception<span class="w"> </span>--ros-args<span class="w"> </span>-p<span class="w"> </span>model_path:<span class="o">=</span>model/resnet18_224x224_nv12.bin<span class="w"> </span>-p<span class="w"> </span>model_name:<span class="o">=</span>resnet18_224x224_nv12
</code></pre></div>
<img alt="run_line_follower" class="img-fluid" src="../../assets/img/deeplearning_line_follower/run_line_follower.png" tag="1" /></p>
<h4 id="start-the-camera"><strong>Start the camera</strong></h4>
<p>First, place the OriginBot in the scene where the line is patrolling.</p>
<p>Use the following command to start the camera driver</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a>ros2<span class="w"> </span>launch<span class="w"> </span>line_follower_perception<span class="w"> </span>usb_cam_web.launch.py
</code></pre></div>
<p>After the camera is successfully started, you can see the dynamically recognized path line position in the line patrol terminal:</p>
<p><img alt="image-20220822151357113" class="img-fluid" src="../../assets/img/deeplearning_line_follower/2022-09-15_21-30.png" tag="1" /></p>
<h4 id="start-the-bot"><strong>Start the bot</strong></h4>
<p>Finally, start the OriginBot chassis again, and the robot can autonomously find the line!</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a>ros2<span class="w"> </span>launch<span class="w"> </span>originbot_bringup<span class="w"> </span>originbot.launch.py
</code></pre></div>
<p><img alt="deeplearning_follow_line" src="../../assets/img/deeplearning_line_follower/deeplearning_follow_line.gif" /></p>
<details class="reference" open="open">
<summary>Reference</summary>
<ul>
<li><a href="https://developer.d-robotics.cc/rdk_doc/en/Application_case/line_follower">Line Following</a></li>
<li><a href="https://developer.d-robotics.cc/forumDetail/251934919646096384">D-robotics X5 OpenExplorer</a></li>
</ul>
</details>
<p><a href="https://www.guyuehome.com/" target="_blank"><img alt="图片1" src="../../assets/img/footer.png" /></a></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      © 2021 古月居 鄂ICP备18024451号-2
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://www.guyuehome.com" target="_blank" rel="noopener" title="www.guyuehome.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M192 32c0 17.7 14.3 32 32 32 123.7 0 224 100.3 224 224 0 17.7 14.3 32 32 32s32-14.3 32-32C512 128.9 383.1 0 224 0c-17.7 0-32 14.3-32 32m0 96c0 17.7 14.3 32 32 32 70.7 0 128 57.3 128 128 0 17.7 14.3 32 32 32s32-14.3 32-32c0-106-86-192-192-192-17.7 0-32 14.3-32 32m-96 16c0-26.5-21.5-48-48-48S0 117.5 0 144v224c0 79.5 64.5 144 144 144s144-64.5 144-144-64.5-144-144-144h-16v96h16c26.5 0 48 21.5 48 48s-21.5 48-48 48-48-21.5-48-48z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://class.guyuehome.com" target="_blank" rel="noopener" title="class.guyuehome.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M249.6 471.5c10.8 3.8 22.4-4.1 22.4-15.5V78.6c0-4.2-1.6-8.4-5-11C247.4 52 202.4 32 144 32 93.5 32 46.3 45.3 18.1 56.1 6.8 60.5 0 71.7 0 83.8v370.3c0 11.9 12.8 20.2 24.1 16.5C55.6 460.1 105.5 448 144 448c33.9 0 79 14 105.6 23.5m76.8 0C353 462 398.1 448 432 448c38.5 0 88.4 12.1 119.9 22.6 11.3 3.8 24.1-4.6 24.1-16.5V83.8c0-12.1-6.8-23.3-18.1-27.6C529.7 45.3 482.5 32 432 32c-58.4 0-103.4 20-123 35.6-3.3 2.6-5 6.8-5 11V456c0 11.4 11.7 19.3 22.4 15.5"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://space.bilibili.com/386919562" target="_blank" rel="noopener" title="space.bilibili.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M488.6 104.1c16.7 18.1 24.4 39.7 23.3 65.7v202.4c-.4 26.4-9.2 48.1-26.5 65.1-17.2 17-39.1 25.9-65.5 26.7H92.02c-26.45-.8-48.21-9.8-65.28-27.2C9.682 419.4.767 396.5 0 368.2V169.8c.767-26 9.682-47.6 26.74-65.7C43.81 87.75 65.57 78.77 92.02 78h29.38L96.05 52.19c-5.75-5.73-8.63-13-8.63-21.79 0-8.8 2.88-16.06 8.63-21.797C101.8 2.868 109.1 0 117.9 0q13.2 0 21.9 8.603L213.1 78h88l74.5-69.397C381.7 2.868 389.2 0 398 0q13.2 0 21.9 8.603c5.7 5.737 8.6 12.997 8.6 21.797 0 8.79-2.9 16.06-8.6 21.79L394.6 78h29.3c26.4.77 48 9.75 64.7 26.1m-38.8 69.7c-.4-9.6-3.7-17.4-10.7-23.5-5.2-6.1-14-9.4-22.7-9.8H96.05c-9.59.4-17.45 3.7-23.58 9.8-6.14 6.1-9.4 13.9-9.78 23.5v194.4c0 9.2 3.26 17 9.78 23.5s14.38 9.8 23.58 9.8H416.4c9.2 0 17-3.3 23.3-9.8s9.7-14.3 10.1-23.5zm-264.3 42.7c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.2 6.3-14 9.5-23.6 9.5s-17.5-3.2-23.6-9.5-9.4-14-9.8-23.2v-33.3c.4-9.1 3.8-16.9 10.1-23.2s13.2-9.6 23.3-10c9.2.4 17 3.7 23.3 10m191.5 0c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2s-14 9.5-23.6 9.5-17.4-3.2-23.6-9.5c-7-6.3-9.4-14-9.7-23.2v-33.3c.3-9.1 3.7-16.9 10-23.2s14.1-9.6 23.3-10c9.2.4 17 3.7 23.3 10"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://gitee.com/guyuehome" target="_blank" rel="noopener" title="gitee.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/guyuehome" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://developer.d-robotics.cc/" target="_blank" rel="noopener" title="developer.d-robotics.cc" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M392.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6m80.6 120.1c-12.5 12.5-12.5 32.8 0 45.3l89.3 89.4-89.4 89.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l112-112c12.5-12.5 12.5-32.8 0-45.3l-112-112c-12.5-12.5-32.8-12.5-45.3 0zm-306.7 0c-12.5-12.5-32.8-12.5-45.3 0l-112 112c-12.5 12.5-12.5 32.8 0 45.3l112 112c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l89.4-89.4c12.5-12.5 12.5-32.8 0-45.3"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["search.highlight", "content.code.copy"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../assets/js/baidu-tongji.js"></script>
      
        <script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
      
        <script src="../../assets/js/simpleLightbox.min.js"></script>
      
        <script src="../../assets/js/extensions.js"></script>
      
        <script src="../../assets/js/custom.js"></script>
      
    
  </body>
</html>